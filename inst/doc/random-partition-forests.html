<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Random Partition Forests</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Random Partition Forests</h1>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(rnndescent)</span></code></pre></div>
<p>The Nearest Neighbor Descent method as usually described is
technically a way to optimize an <em>existing</em> estimate of the
nearest neighbor graph. You must think of a way to initialize the graph.
The obvious approach and the one used in the description of NND in <span class="citation">(Dong, Moses, and Li 2011)</span> is to start with a
random selection of neighbors. One of the clever things about the
PyNNDescent implementation is that it uses a random partition forest
<span class="citation">(Dasgupta and Freund 2008)</span> to come up with
the initial guess. Random partition forests are part of a large group of
tree-based methods. These are often very fast and conceptually simple,
but can be inaccurate. Much of the literature is devoted to proposals of
tweaks to these methods to improve their performance, often at the
expense of their simplicity and speed. PyNNDescent (and rnndescent
follows its lead) avoids this because we only need to get to a decent
guess of the nearest neighbor graph which we can then improve by nearest
neighbor descent. As long as we don’t take substantially longer than the
random initialization to come up with the guess and it’s sufficiently
good, we should come out ahead.</p>
<div id="random-partition-forests" class="section level2">
<h2>Random Partition Forests</h2>
<p>Here’s a basic introduction to how random partition forests work.</p>
<div id="building-a-space-partitioning-tree" class="section level3">
<h3>Building a Space-Partitioning Tree</h3>
<p>First, we will consider the recipe for building a space-partitioning
tree:</p>
<ol style="list-style-type: decimal">
<li>Select a dimension.</li>
<li>Select a split point along that dimension.</li>
<li>Split the data into two child nodes based on the split point.</li>
<li>Repeat steps 1-3 on each of the two groups.</li>
<li>When the number of items in a group is less than some threshold, the
node is now a leaf, and stop splitting.</li>
</ol>
<p>Variations of steps 1 and 2 determines the vast majority of the
differences between the various tree-based methods.</p>
</div>
<div id="building-a-random-partition-tree" class="section level3">
<h3>Building a Random Partition Tree</h3>
<p>For a random partition tree we:</p>
<ol style="list-style-type: decimal">
<li>Select two points at random.</li>
<li>Calculate the mid-point between those two points.</li>
</ol>
<p>This is enough to define a hyperplane in the data. This is not
<em>exactly</em> the algorithm as described in <span class="citation">(Dasgupta and Freund 2008)</span>, but it is how it’s
done in the very similar method <a href="https://github.com/spotify/annoy">Annoy</a>.</p>
<p>Step 3 then involves calculating which side of the hyperplane each
point is on and assigning data to the child nodes on that basis.</p>
</div>
<div id="from-trees-to-forests" class="section level3">
<h3>From Trees to Forests</h3>
<p>A random partition forest is just a collection of random partition
trees. Because of the random nature of the trees, they will all be
different.</p>
</div>
</div>
<div id="build-a-forest" class="section level2">
<h2>Build a Forest</h2>
<p>To build a forest with <code>rnndescent</code>, use the
<code>rpf_build</code> function. We’ll use the <code>iris</code> dataset
as an example, with the goal of finding the 15-nearest neighbors of each
item in the dataset.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>iris_forest <span class="ot">&lt;-</span> <span class="fu">rpf_build</span>(iris, <span class="at">leaf_size =</span> <span class="dv">15</span>)</span></code></pre></div>
<p>Some options at your disposal:</p>
<ul>
<li><code>metric</code>: the type of distance calculation to use. The
default is <code>euclidean</code>, but there are a lot to choose from.
See the help text for the <code>metric</code> parameter in rpf_build()`
for details.</li>
<li><code>n_trees</code>: the number of trees to build. The default is
to choose based on the size of the data provided, with a maximum of 32:
eventually you will get diminishing returns from the number of trees in
a forest.</li>
<li><code>leaf_size</code>: the number of items in a leaf. The splitting
procedure stops when there are fewer than this number of items in a
node. The default is <code>10</code> but you will want the leaf size to
scale with the number of neighbors you will look for, so I have
increased it to <code>15</code> for this example. The bigger this value
the more accurate the search will be, but at the cost of a lot more
distance calculations to carry out. Conversely, if you make it too small
compared to the number of neighbors, then you may end up with not all
items finding <code>k</code> neighbors.</li>
<li><code>max_tree_depth</code>: the maximum depth of the tree. If a
tree reaches this depth then even if the current node size exceeds the
value of <code>leaf_size</code>, it will stop splitting. The point of
splitting a tree is that the size of each leaf <em>should</em> rapidly
decrease as you go down the tree, and in an ideal case it would decrease
by a factor of two at each level, so ideally we can process datasets
that vary by many orders of magnitude while the depth of the tree only
increases by a few levels. The default <code>max_tree_depth</code> is
200, so if you trigger this limit, the answer may <em>not</em> be to
increase the depth. It’s more likely that there is something about the
distribution of your data that prevents it from splitting well. In this
case, if there’s a different <code>metric</code> to try that still has
relevance for your data, that’s worth a try, but possibly the best
solution is to abandon the tree-based approach (for example initialize
nearest neighbor descent with random neighbors). If you set
<code>verbose = TRUE</code> you will get a warning about the maximum
leaf size being larger than <code>leaf_size</code>.</li>
<li><code>margin</code>: this makes a slight modification to how the
assignment of data to the sides of the hyperplane is calculated. We’ll
discuss this below.</li>
</ul>
<p>The forest that is returned is just an R list, so you can save it and
load it with <code>saveRDS</code> and <code>readRDS</code> without
issue. But it’s not something you will want to inspect and definitely
don’t modify it. It’s mainly useful for passing to other functions, like
the one we will talk about next.</p>
</div>
<div id="finding-nearest-neighbors" class="section level2">
<h2>Finding Nearest Neighbors</h2>
<p>To use this to find nearest neighbors, a query point will traverse
the tree from the root to a leaf, calculating the side of each
hyperplane it encounters. All the items in the leaf in which it ends up
are then candidates for nearest neighbors.</p>
<p>To query the forest we just build, we use the
<code>rpf_knn_query</code> function. Apart from the forest itself, we
also need the data we want to query (<code>query</code>) and the data
used to build the forest (<code>reference</code>), because the forest
doesn’t store that information. In thus case, because we are looking at
the k-nearest neighbors or <code>iris</code>, the <code>query</code> and
the <code>reference</code> are the same, but they don’t have to be. At
this point, we must also specify the number of neighbors we want.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>iris_query <span class="ot">&lt;-</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>  <span class="fu">rpf_knn_query</span>(</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="at">query =</span> iris,</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    <span class="at">reference =</span> iris,</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    <span class="at">forest =</span> iris_forest,</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    <span class="at">k =</span> <span class="dv">15</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>  )</span></code></pre></div>
<p>The <code>iris_query</code> that is returned is a list with two
matrices: <code>idx</code> contains for each row the indices of the
k-nearest neighbors, and <code>dist</code> contains the distances.</p>
</div>
<div id="a-small-optimization-for-the-k-nearest-neighbors" class="section level2">
<h2>A Small Optimization for the k-Nearest Neighbors</h2>
<p>You could use the querying approach mentioned above for finding the
k-nearest neighbors of the data that was used in building the tree.
However, the data has already been partitioned so if you want k-nearest
neighbor data, there’s a more efficient way to do that: for each leaf,
the k-nearest neighbors of each point in the leaf are the other members
of that leaf. While usually the distance calculations take up most of
the time when looking for neighbors, you do avoid having to make any
tree traversals and the associated hyperplane distance calculations.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>iris_knn <span class="ot">&lt;-</span> <span class="fu">rpf_knn</span>(iris, <span class="at">k =</span> <span class="dv">15</span>)</span></code></pre></div>
<p>This should give the same result as running <code>rpf_build</code>
followed by <code>rpf_knn_query</code> (apart from the vagaries of the
random number generator), but is a lot more convenient and a bit faster.
You have access to the same parameters for forest building as
<code>rpf_build</code>, e.g. <code>leaf_size</code>,
<code>n_trees</code>, <code>max_tree_depth</code> etc.</p>
<p>Additionally, if you want the k-nearest neighbors <em>and</em> you
also want the forest for future querying, if you set
<code>ret_forest = TRUE</code>, the return value will now also contain
the forest as the <code>forest</code> item in the list. In this example
we build the forest (and get the 15-nearest neighbors) for the first 50
<code>iris</code> items and then query the remaining 100:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>iris_knn_with_forest <span class="ot">&lt;-</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  <span class="fu">rpf_knn</span>(iris[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, ], <span class="at">k =</span> <span class="dv">15</span>, <span class="at">ret_forest =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>iris_query_virginica <span class="ot">&lt;-</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  <span class="fu">rpf_knn_query</span>(</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="at">query =</span> iris[<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>, ],</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="at">reference =</span> iris[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, ],</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    <span class="at">forest =</span> iris_knn_with_forest<span class="sc">$</span>forest,</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    <span class="at">k =</span> <span class="dv">15</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  )</span></code></pre></div>
</div>
<div id="margin" class="section level2">
<h2>Margin</h2>
<p>The <code>margin</code> parameter determines how to calculate the
side of the hyperplane each item in a split belongs to. The usual method
(<code>margin = &quot;explicit&quot;</code>) does the same thing as in
PyNNDescent: the way the hyperplane is defined is to use the vector
defined by the two points <span class="math inline">\(a\)</span> and
<span class="math inline">\(b\)</span> as the normal vector to a plane,
and then the point midway between them as the point on the plane. We
then calculate the margin of a point <span class="math inline">\(x\)</span> (effectively the signed distance from
the plane to <span class="math inline">\(x\)</span>) as:</p>
<p><span class="math display">\[
\text{margin}(\mathbf{x}) = ((\mathbf{b} - \mathbf{a}) \cdot (\mathbf{x}
- \frac{\mathbf{a} + \mathbf{b}}{2}))
\]</span></p>
<p>Taking dot products of vectors and finding mid points is all totally
unexceptional if you are using a Euclidean metric. And because there is
a monotonic relationship between the cosine distances and the Euclidean
distance after normalization of vectors, we can define an “angular”
version of this calculation that works on the normalized vectors.</p>
<p>But for some datasets this will be a bit weird and un-natural.
Imagine a dataset of binary vectors in which you are applying e.g. the
Hamming metric. The mid-point of two binary vectors is not a binary
vector, and nor does it make sense to think about the geometric
relationship implied by a dot product.</p>
<p>As an alternative to calculating the margin via an explicit creation
of a hyperplane, you could instead think about how the distance between
<span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>, <span class="math inline">\(d_{xa}\)</span> compares to the distance between
<span class="math inline">\(x\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(d_{xb}\)</span> and what the significance for the
margin is. Remember that the vector defined by <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the normal vector to the hyperplane,
so you can think of a line connecting <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, with the hyperplane splitting that
line in two equal halves. Now imagine <span class="math inline">\(x\)</span> is somewhere on that line. If <span class="math inline">\(x\)</span> is closer to <span class="math inline">\(a\)</span> than <span class="math inline">\(b\)</span> it must be on the same side of the
hyperplane as <span class="math inline">\(a\)</span>, and vice versa.
Therefore we can calculate the margin by comparing <span class="math inline">\(d_{xa}\)</span> and <span class="math inline">\(d_{xb}\)</span> and seeing which value is
smaller.</p>
<p>Because we don’t explicitly create the hyperplane, I call this the
“implicit” margin method and you can choose to generate splits this way
by setting <code>margin = &quot;implicit&quot;</code>. We’ll use some random
binary data for this example.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>binary_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">as.logical</span>(<span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)), <span class="at">ncol =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Note the <code>as.logical</code> call: if <code>rnndescent</code>
detects binary data in this format <em>and</em> you specify a metric
which is appropriate for binary data (e.g. Hamming), <em>and</em> you
use <code>margin = &quot;implicit&quot;</code> then a specialized function is
called which should be much faster than the functions written only with
generic floating point data in mind.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>bin_knn_imp <span class="ot">&lt;-</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  <span class="fu">rpf_knn</span>(binary_data,</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    <span class="at">k =</span> <span class="dv">15</span>,</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    <span class="at">metric =</span> <span class="st">&quot;hamming&quot;</span>,</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    <span class="at">margin =</span> <span class="st">&quot;implicit&quot;</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>  )</span></code></pre></div>
<p>The following will give the same results but for large datasets is
likely to be noticeably slower:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>bin_knn_exp <span class="ot">&lt;-</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>  <span class="fu">rpf_knn</span>(binary_data,</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="at">k =</span> <span class="dv">15</span>,</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="at">metric =</span> <span class="st">&quot;hamming&quot;</span>,</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="at">margin =</span> <span class="st">&quot;explicit&quot;</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>  )</span></code></pre></div>
<p>So if the implicit margin method is faster (and makes sense for more
metrics) why would you ever want to use the explicit method? Well, the
implicit method is only faster for binary data with specialized metrics.
The downside of the implicit method is that determining the side of the
hyperplane requires <em>two</em> distance calculations per point,
whereas the explicit method only requires the dot product calculation,
which is likely to be only as costly as a single distance calculation.
So for floating point data, the explicit method is likely to be about
twice as fast. That’s a lot to think about so the default setting for
<code>margin</code> is <code>&quot;auto&quot;</code>, which tries to do the right
thing: if you are using binary data with a suitable metric, it will use
the implicit method, otherwise it will use the explicit method and
normalize the vectors to give a more “angular” approach for some metrics
that put more emphasis on angle versus magnitude.</p>
</div>
<div id="filtering-a-forest" class="section level2">
<h2>Filtering a Forest</h2>
<p>As mentioned at the beginning of this vignette, in
<code>rnndescent</code> it’s expected that you would only use random
partition forests as an initialization to nearest neighbor descent. In
that case, keeping the entire forest for querying new data is probably
unnecessary: we can keep only the “best” trees. PyNNDescent only keeps
one tree for this purpose. For determining what tree is “best”, we mean
the tree that reproduces the k-nearest neighbor graph most effectively.
You can do this by comparing an existing k-nearest neighbor graph with
that produced by a single tree. The <code>rpf_filter</code> function
does this for you:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>iris_filtered <span class="ot">&lt;-</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>  <span class="fu">rpf_filter</span>(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    <span class="at">nn =</span> iris_query,</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    <span class="at">forest =</span> iris_forest,</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    <span class="at">n_trees =</span> <span class="dv">1</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>  )</span></code></pre></div>
<p><code>n_trees</code> is the number of trees to keep. Feel free to
keep more if you like, although there is no extra diversification step
to ensure that the trees being retained are both good at reproducing the
k-nearest neighbor graph <em>and</em> are diverse from each other
(perhaps they reproduce different parts of the neighbor graph well?).
The higher quality the k-nearest-neighbor graph is, the better the
filtering will work so although the example above uses the graph from
the forest, you might get better results using the graph from having run
nearest neighbor descent with the forest result as input.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-dasgupta2008random" class="csl-entry">
Dasgupta, Sanjoy, and Yoav Freund. 2008. <span>“Random Projection Trees
and Low Dimensional Manifolds.”</span> In <em>Proceedings of the
Fortieth Annual ACM Symposium on Theory of Computing</em>, 537–46.
</div>
<div id="ref-dong2011efficient" class="csl-entry">
Dong, Wei, Charikar Moses, and Kai Li. 2011. <span>“Efficient k-Nearest
Neighbor Graph Construction for Generic Similarity Measures.”</span> In
<em>Proceedings of the 20th International Conference on World Wide
Web</em>, 577–86.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
