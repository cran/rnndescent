<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Nearest Neighbor Descent</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Nearest Neighbor Descent</h1>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(rnndescent)</span></code></pre></div>
<p>Nearest Neighbor Descent <span class="citation">(Dong, Moses, and Li
2011)</span> (NND) is the main way to construct a k-nearest neighbors
graph in <code>rnndescent</code>. Here’s a brief description of the
method.</p>
<p>The idea behind NND is to start with an initial guess of the graph
(typically randomly chosen neighbors) and then iteratively improving
that guess by taking candidate neighbors which are neighbors of
neighbors. For example: if an item <code>i</code> has a current neighbor
<code>j</code>, then <code>j</code>’s neighbors are candidates for
<code>i</code>’s neighbors. The “descent” part is in analogy with
gradient descent, where you can see the sum of the distances in the
graph as an objective function: as better neighbors enter the graph, the
distances must get smaller.</p>
<p>Conceptually it would seem that you would implement this algorithm
with a loop like the following in each iteration:</p>
<ol style="list-style-type: decimal">
<li>For each item <code>i</code> in the graph:</li>
<li>For each item <code>j</code> in the neighbors of
<code>i</code>:</li>
<li>For each item <code>k</code> in the neighbors of
<code>j</code>:</li>
<li>If <code>k</code> is not already a neighbor of <code>i</code>:</li>
<li>Calculate the distance between <code>i</code> and <code>k</code>,
<span class="math inline">\(d_{ik}\)</span>.</li>
<li>If <span class="math inline">\(d_{ik}\)</span> is smaller than the
neighbor with the largest distance in the neighbor list of <span class="math inline">\(i\)</span>, update the neighbor list of
<code>i</code> with <code>k</code>.</li>
</ol>
<div id="local-join" class="section level2">
<h2>Local Join</h2>
<p>The process described above involves a lot of looping and repeated
fetching of neighbor vectors, so NND actually uses the concept of a
“local join”. One way to think of it is to consider an item
<code>i</code> fielding requests for its nearest neighbors. It will be
repeatedly asked for it by any other item which considers it a neighbor.
So if we did some work at the start of each iteration to know all the
items which consider <code>i</code> a neighbor, we can generate all the
candidates neighbor pairs that <code>i</code> is involved with at once.
Then we only need to iterate over the items in the graph. We do need to
do the work of finding out who considers <code>i</code> a neighbor but
that also only requires a loop over the graph also.</p>
<p>To be clear, the same amount of work needs to be done, but by doing
it in a different order, everything is a bit more efficient in terms of
what needs to be fetched from memory.</p>
<p>The up-shot of using the local join approach is that rather than
iterating over the graph one item at a time, we end up a list of pairs
of items <code>(i, j)</code> to update the graph as a whole with. And
because we are dealing with a kNN graph if we have a pair
<code>(i, j)</code> we also have <code>(j, i)</code> as a potential
update, at the cost of only one distance calculation. This has some
challenges in terms of parallel implementation and it also makes caching
distances a bit harder but it’s still better than the more naive
approach of explicitly looping over all neighbors-of-neighbors.</p>
</div>
<div id="other-heuristics" class="section level2">
<h2>Other Heuristics</h2>
<p>Additionally, there are two other heuristics used to reduce the
amount of work done. The first is that candidate neighbors are split
into “new” and “old” candidates. A “new” candidate neighbor is any
neighbor which entered the graph in the previous iteration. “Old”
neighbors are everything else. For the local join, all possible pairs of
“new” neighbors are used for updating the graph, but “old” neighbors are
only ever paired with “new” neighbors, not other “old” neighbors. This
is referred to as “incremental search” in the NND paper.</p>
<p>Also, a tolerance <span class="math inline">\(\delta\)</span> is used
to determine as an early stopping criterion. The total number of items
in the graph is <span class="math inline">\(kN\)</span> where <span class="math inline">\(k\)</span> is the number of neighbors and <span class="math inline">\(N\)</span> is the number of items. During each
iteration, a counter is incremented every time the graph is successfully
updated. If at the end of the iteration the number of updates is less
than <span class="math inline">\(\delta kN\)</span> then the iteration
stops.</p>
</div>
<div id="pynndescent-modifications" class="section level2">
<h2>PyNNDescent Modifications</h2>
<p>There is one other minor change to how PyNNDescent works versus the
description in the NND paper, which <code>rnndescent</code> also uses,
which is how sampling of candidates works. For the local join, we need
to know not just the neighbors of <code>i</code>, but those items which
consider <code>i</code> a neighbor, which we call the “reverse
neighbors” of <code>i</code>. While there are always only <span class="math inline">\(k\)</span> “forward” neighbors of <code>i</code>
in a graph, we don’t control who is a neighbor of what, so
<code>i</code> could be the neighbor of many (or even all) the other
items in a dataset. Thus, building the reverse list can be a bit
challenging as we need to be prepared for any item to have up to <span class="math inline">\(N\)</span> neighbors. In the NND paper, this is
avoided by defining a sample rate <span class="math inline">\(\rho\)</span>, which is used to sample from the
k-nearest neighbors, and then the reverse neighbor list is only built
from the sampled items. A subsequent down-sampling is then applied to
the reverse neighbor list so that both the forward and reverse neighbor
list only contain <span class="math inline">\(\rho k\)</span> items.</p>
<p>Instead of a sample rate, <code>rnndescent</code> defines a
<code>max_candidates</code> parameter determines the size of both the
forward and reverse neighbor lists per item. If there are more
candidates than the <code>max_candidates</code> value, the retained
candidates are chosen randomly so this works like random sampling.</p>
<p>Finally, instead of a random initialization, PyNNDescent uses a
k-nearest neighbors graph from a random projection forest. There is an
entire vignette explaining how RP forest works. This is also an option
in <code>rnndescent</code>.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>It’s easy enough to run NND on a dataset. Here’s an example using the
<code>iris</code> dataset:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>iris_knn <span class="ot">&lt;-</span> <span class="fu">nnd_knn</span>(iris, <span class="at">k =</span> <span class="dv">15</span>)</span></code></pre></div>
<p>The contents of <code>iris_knn</code> is a list with two elements,
both <span class="math inline">\(N\)</span> by <span class="math inline">\(k\)</span> matrices where <span class="math inline">\(N\)</span> is the number of items in the dataset
and <span class="math inline">\(k\)</span> is the number of neighbors:
<code>idx</code> contains the indices of the neighbors:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>iris_knn<span class="sc">$</span>idx[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#&gt; [1,]    1   18    5   29   40</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">#&gt; [2,]    2   13   46   35   10</span></span></code></pre></div>
<p>and <code>dist</code> contains the distances:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>iris_knn<span class="sc">$</span>dist[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co">#&gt;      [,1]      [,2]      [,3]      [,4]      [,5]</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co">#&gt; [1,]    0 0.1000000 0.1414212 0.1414212 0.1414213</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#&gt; [2,]    0 0.1414213 0.1414213 0.1414213 0.1732050</span></span></code></pre></div>
<p>Apart from <code>k</code>, there are some parameters you may want to
modify:</p>
<ul>
<li><code>metric</code> is the distance metric to use. The default is
Euclidean distance. There are several metrics you can use. See the
documentation for <code>nnd_knn</code> for the full list.</li>
<li><code>init</code> is the initialization method. The default is
<code>&quot;rand&quot;</code> which initializes the neighbors randomly. You may
wish to use <code>&quot;tree&quot;</code> which uses a random projection forest to
initialize the neighbors, similar to <code>rpf_build</code>. To control
the tree building, you can pass the same sort of parameters that you
would to <code>rpf_build</code> via the <code>init_args</code>
parameter. See the vignette on RP forest for more details. You can also
pass in a neighbor graph directly. This should have the same format as
the output of <code>nnd_knn</code>, i.e. a list of two matrices of size
<span class="math inline">\(N\)</span> by <span class="math inline">\(k\)</span>. NND can be used to refine an existing
graph generated by other methods, e.g. <a href="https://cran.r-project.org/package=RcppAnnoy">RcppAnnoy</a> or <a href="https://cran.r-project.org/package=RcppHNSW">RcppHNSW</a>.</li>
<li><code>n_iters</code> is the number of iterations of NND to carry
out. The default is to choose based on <span class="math inline">\(N\)</span>, the number of items in the dataset.
The amount of work done per iteration decreases quite rapidly, so
sticking with the default is usually sensible, especially if you don’t
change the convergence criterion <code>delta</code> (see below), because
this often causes the algorithm to stop early anyway.</li>
<li><code>delta</code> controls early stopping and must be a value
between <code>0</code> and <code>1</code>. If in a given iteration, the
number of changes to the neighbor graph is less than
<code>delta * k * N</code> then the algorithm stops. The default is
<code>0.001</code> so you can interpret that roughly as the neighbor
graph needs to have changed by 0.1% to avoid early stopping.</li>
<li><code>max_candidates</code> controls the size of the forward and
reverse neighbor lists. The default is to set this to whatever is
smaller, <code>k</code> or <code>60</code>.</li>
<li><code>n_threads</code> controls the number of threads to use. The
default is to run as a single thread. The slow part of any approximate
nearest neighbor algorithm is the distance calculation so using multiple
threads is usually a good idea.</li>
<li><code>ret_forest</code> if <code>TRUE</code>, and you have set
<code>init = &quot;tree&quot;</code>, then the random projection forest used to
initialize the neighbor graph is returned as well. If you want to
generate new neighbors based on the original data you will want
this.</li>
<li><code>verbose</code> set to <code>TRUE</code> to get information
about the progress of the NND.</li>
<li><code>progress</code> this affects how the progress of NND is
displayed when <code>verbose = TRUE</code>. The default <code>bar</code>
shows a textual progress bar. You can also set
<code>progress = &quot;dist&quot;</code> to show the current value of the
convergence criterion and the sum of the distances at each iteration.
This can help a bit to determine if more iterations or a different
convergence criterion will help.</li>
</ul>
<p>Note that NND uses random number generation to determine the order of
processing candidates, so for reproducible results you should set the
random number seed explicitly. Also, the way that parallelism is
implemented means that reproducibility is not possible for different
settings of <code>n_threads</code> even with a consistent seed,
e.g. going from <code>n_threads = 0</code> to <code>n_threads = 4</code>
will give you different results, even if you <code>set.seed</code> with
a fixed seed beforehand.</p>
</div>
<div id="troubleshooting" class="section level2">
<h2>Troubleshooting</h2>
<p>If you have reason to believe you aren’t getting the results out of
NND that are sufficiently accurate, probably the best thing to do is to
increase <code>max_candidates</code>. Reducing <code>delta</code> or
increasing <code>n_iters</code> usually has less effect. Restarting
<code>nnd_knn</code> with <code>init</code> set to the output of your
previous run usually also helps, but is not a very time-efficient way to
improve matters.</p>
<p>Here is some (lightly edited) sample output when running</p>
<pre><code>iris_knn &lt;- nnd_knn(iris, k = 15, verbose = TRUE, progress = &quot;dist&quot;)</code></pre>
<pre><code>Running nearest neighbor descent for 7 iterations
1 / 7
heap sum = 647.85 num_updates = 3356 tol = 2.25
2 / 7
heap sum = 599.9 num_updates = 216 tol = 2.25
3 / 7
heap sum = 599.9 num_updates = 0 tol = 2.25
Convergence: c = 0 tol = 2.25</code></pre>
<p>This tells you that for a dataset of the size of <code>iris</code>,
at most 7 iterations will run. The <code>1 / 7</code>,
<code>2 / 7</code> and so on is logged at the end of each iteration.
Following that is the sum of the distances of the neighbors in the heap,
the number of updates to the neighbor graph and the convergence
criterion. If <code>num_updates</code> falls below <code>tol</code> then
the algorithm stops. In this case, on the third iteration there were no
updates at all, so the algorithm stopped early.</p>
<p>In this case, almost certainly NND has found the exact nearest
neighbors, so you wouldn’t be worried about modifying the parameters.
But if you were so inclined, the output shows you that there would be
little point in increasing <code>n_iters</code> or reducing
<code>delta</code>. This really only leaves <code>max_candidates</code>
as an option.</p>
<p>The vignette on dealing with <a href="hubness.html">hubness</a>
(where this can be an issue) goes into a bit more detail on how to use
different functions in <code>rnndescent</code> to deal with this sort of
problem.</p>
</div>
<div id="querying-new-data" class="section level2">
<h2>Querying New Data</h2>
<p>You can’t. NND can only produce the k-nearest neighbors graph for the
provided data. It doesn’t produce an “index” of any kind that you can
query. The value of NND and the local join really only makes sense if
you can take advantage of the fact that calculating the distance <span class="math inline">\(d_{ij}\)</span> lets update the neighbor list of
<span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> at once.</p>
<p>If you try to apply the concepts from NND to querying new data you
quickly end up at a method that looks a lot like most greedy graph-based
searches. For that, you should look at <code>graph_knn_query()</code>,
although as noted above you can also use the random projection forest
used to initialize the neighbor graph when <code>init = &quot;tree&quot;</code>.
You will also probably want to augment the neighbor graph to make it
more amenable for searching using
<code>prepare_search_graph()</code>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-dong2011efficient" class="csl-entry">
Dong, Wei, Charikar Moses, and Kai Li. 2011. <span>“Efficient k-Nearest
Neighbor Graph Construction for Generic Similarity Measures.”</span> In
<em>Proceedings of the 20th International Conference on World Wide
Web</em>, 577–86.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
